{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating hrrs\n",
    "def hrr(length, normalized=False):\n",
    "    if normalized:\n",
    "        x = np.random.uniform(-np.pi,np.pi,int((length-1)/2))\n",
    "        if length % 2:\n",
    "            x = np.real(np.fft.ifft(np.concatenate([np.ones(1), np.exp(1j*x), np.exp(-1j*x[::-1])])))\n",
    "        else:\n",
    "            x = np.real(np.fft.ifft(np.concatenate([np.ones(1), np.exp(1j*x), np.ones(1), np.exp(-1j*x[::-1])])))\n",
    "    else:\n",
    "        x = np.random.normal(0.0, 1.0/np.sqrt(length), length)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve two hrrs\n",
    "def convolve(x, y):\n",
    "    return np.real(np.fft.ifft(np.fft.fft(x)*np.fft.fft(y)))\n",
    "\n",
    "# Pre convolve all hrrs to save time\n",
    "def preconvolve():\n",
    "    preconvolved_matrix = np.zeros([possible_wm.size, size_of_maze, n])\n",
    "    for x in range(len(goal_states)):\n",
    "        for y in range(size_of_maze):\n",
    "            preconvolved_matrix[x][y] = convolve(possible_wm[x], state_hrrs[y])\n",
    "    return preconvolved_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching between tasks\n",
    "def switch_task(count):\n",
    "    if count < 9:\n",
    "        goal = goal_states[0]\n",
    "        count += 1\n",
    "    elif count == 9:\n",
    "        goal = goal_states[1]\n",
    "        count += 1\n",
    "    elif count > 9 and count < 19:\n",
    "        goal = goal_states[1]\n",
    "        count += 1\n",
    "    elif count == 19:\n",
    "        goal = goal_states[0]\n",
    "        count = 0\n",
    "    return goal, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy for choosing left or right\n",
    "def policy_mov(state, wm, rand_on):\n",
    "    \n",
    "    # Calculates what the left and right positions on the maze are\n",
    "    if (state == 0):\n",
    "        state_left = size_of_maze - 1\n",
    "        state_right = state + 1\n",
    "    elif (state == size_of_maze - 1):\n",
    "        state_left = state - 1\n",
    "        state_right = 0\n",
    "    else:\n",
    "        state_left = state - 1\n",
    "        state_right = state + 1\n",
    "    \n",
    "    # Value of each move\n",
    "    value_left_state_wm = np.dot(weights, preconvolved_matrix[wm][state_left]) + bias\n",
    "    value_right_state_wm = np.dot(weights, preconvolved_matrix[wm][state_right]) + bias\n",
    "    \n",
    "    # Random move\n",
    "    if((np.random.random() < e_soft) and (rand_on == 1)):\n",
    "        return np.random.choice([state_left, state_right])\n",
    "    \n",
    "    # Finds best move\n",
    "    max_value = max(value_left_state_wm, value_right_state_wm)\n",
    "    \n",
    "    # Returns best move\n",
    "    if(max_value == value_left_state_wm):\n",
    "        return state_left\n",
    "    elif(max_value == value_right_state_wm):\n",
    "        return state_right\n",
    "    \n",
    "# Switching wm depending on td error\n",
    "def policy_switch(wm):\n",
    "    return (wm + 1)%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of hrrs\n",
    "n = 1024\n",
    "\n",
    "# Maze creation\n",
    "size_of_maze = 5\n",
    "\n",
    "# Reward for learning\n",
    "reward_bad = 0\n",
    "reward_good = 1\n",
    "\n",
    "# Discounted future rewards\n",
    "discount = 0.6\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.03\n",
    "\n",
    "# Exploration on or off\n",
    "exp_on = 1\n",
    "\n",
    "# Exploration rate\n",
    "e_soft = 0.01\n",
    "\n",
    "# Treshold to switch wm\n",
    "threshold = -0.20\n",
    "\n",
    "# Number of training cycles\n",
    "episodes = 5000\n",
    "\n",
    "# Steps to try finding the goal before quiting\n",
    "steps_till_quit = 100\n",
    "\n",
    "# For switching tasks\n",
    "count = -1\n",
    "\n",
    "# Frequency to print information\n",
    "print_freq = 1000\n",
    "\n",
    "# wm is set to first\n",
    "wm = 0\n",
    "\n",
    "# Neural network\n",
    "weights = hrr(n, True)\n",
    "bias = 0\n",
    "\n",
    "eligibility = [0] * n\n",
    "\n",
    "eli_lambda = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting goals\n",
    "goal_states = random.sample(range(0, size_of_maze), 2)\n",
    "\n",
    "# Create all state hrrs\n",
    "state_hrrs = np.zeros([size_of_maze, n])\n",
    "for x in range(size_of_maze):\n",
    "    state_hrrs[x] = hrr(n, True)\n",
    "    \n",
    "# Create all wm hrrs\n",
    "possible_wm = np.zeros([len(goal_states), n])\n",
    "for x in range(len(goal_states)):\n",
    "    possible_wm[x] = hrr(n, True)\n",
    "\n",
    "# Reward hrr\n",
    "reward_tkn = hrr(n, True)\n",
    "\n",
    "# Pre convolve hrrs to save time\n",
    "preconvolved_matrix = preconvolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Goals:\", goal_states[0], goal_states[1])\n",
    "for x in range(episodes):\n",
    "    \n",
    "    # Current state\n",
    "    current = random.randint(0, size_of_maze - 1)\n",
    "    \n",
    "    # Switching tasks\n",
    "    goal_state, count = switch_task(count)\n",
    "    \n",
    "    eligibility = [0] * n\n",
    "\n",
    "    for y in range (steps_till_quit):\n",
    "        \n",
    "        # Store info about previous state\n",
    "        previous = current\n",
    "        previous_state = preconvolved_matrix[wm][previous]\n",
    "        previous_value = np.dot(weights, previous_state) + bias\n",
    "        \n",
    "        eligibility = [x * eli_lambda for x in eligibility]\n",
    "        \n",
    "        # What state to move into\n",
    "        move = policy_mov(current, wm, exp_on)\n",
    "        \n",
    "        # Make the move\n",
    "        current = move\n",
    "        current_state = preconvolved_matrix[wm][current]\n",
    "        current_value = np.dot(weights, current_state) + bias\n",
    "              \n",
    "        # Goal reached\n",
    "        if (current == goal_state):\n",
    "            # Get temporal difference error and update weights of neural network\n",
    "            error = reward_good - (np.dot(weights, convolve(reward_tkn, current_state)) + bias)\n",
    "            eligibility = eligibility + convolve(reward_tkn, current_state)\n",
    "            weights = np.add(weights, (alpha * error * eligibility))\n",
    "            break\n",
    "\n",
    "        # Weight update for goal not found\n",
    "        error = (reward_bad + discount * current_value) - previous_value\n",
    "        eligibility = eligibility + previous_state\n",
    "        if error < threshold:\n",
    "                wm = policy_switch(wm)\n",
    "                break\n",
    "        weights = np.add(weights, (alpha * error * eligibility))\n",
    "        \n",
    "    if (x+1)%print_freq == 0:\n",
    "        print(\"Episode\", x+1, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.arange(size_of_maze)\n",
    "value = np.zeros(size_of_maze)\n",
    "for z in range (size_of_maze):\n",
    "    if z == goal_states[0]:\n",
    "        value[z] = np.dot(weights, convolve(reward_tkn, preconvolved_matrix[0][z])) + bias\n",
    "    else:\n",
    "        value[z] = np.dot(weights, preconvolved_matrix[0][z]) + bias\n",
    "plt.plot(position, value)\n",
    "print(np.amax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.arange(size_of_maze)\n",
    "value = np.zeros(size_of_maze)\n",
    "for z in range (size_of_maze):\n",
    "    if z == goal_states[1]:\n",
    "        value[z] = np.dot(weights, convolve(reward_tkn, preconvolved_matrix[1][z])) + bias\n",
    "    else:\n",
    "        value[z] = np.dot(weights, preconvolved_matrix[1][z]) + bias\n",
    "plt.plot(position, value)\n",
    "print(np.amax(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
