{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_states = [1, 5]\n",
    "size_of_maze = 10\n",
    "episodes = 40\n",
    "\n",
    "discount = 0.7\n",
    "alpha = 0.6\n",
    "e_soft = 0.05\n",
    "\n",
    "n = 1024\n",
    "mean = 0\n",
    "sigma = 1 / math.sqrt(n)\n",
    "\n",
    "inputs = n\n",
    "ouputs = 1\n",
    "\n",
    "reward_bad = -1\n",
    "reward_good = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_hrrs = np.random.normal(mean, sigma, size=[size_of_maze, n])\n",
    "\n",
    "identity_hrr = np.zeros(n)\n",
    "identity_hrr[0] = 1\n",
    "\n",
    "color_hrrs_internal = np.random.normal(mean, sigma, size=[len(goal_states), n])\n",
    "color_hrrs_external = np.random.normal(mean, sigma, size=[len(goal_states), n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(hrr_1, hrr_2):\n",
    "    convolved_matrix = np.zeros(n).T\n",
    "    for j in range(n):\n",
    "        for k in range(n):\n",
    "            convolved_matrix[j] += hrr_1[k] * hrr_2[j - k]\n",
    "    return convolved_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state_left, state_right, wm, color, signal, rand_on):\n",
    "    # Calculates the value of each possible decision\n",
    "    # Value of stepping left with signal and with signal and wm\n",
    "    value_left_state_x = nn.feedforward(convolve(state_hrrs[state_left], signal))\n",
    "    value_left_state_wm = nn.feedforward(convolve(convolve(state_hrrs[state_left], signal), wm))\n",
    "    \n",
    "    # Value of stepping right with signal and with signal and wm\n",
    "    value_right_state_x = nn.feedforward(convolve(state_hrrs[state_right], signal))\n",
    "    value_right_state_wm = nn.feedforward(convolve(convolve(state_hrrs[state_right], signal), wm))\n",
    "    \n",
    "    # Random move\n",
    "    if((np.random.random() < e_soft) and (rand_on == 1)):\n",
    "        return np.random.choice([-1, 1]), wm\n",
    "    \n",
    "    # Properly sets signal to the repective internal value\n",
    "    if(not (np.array_equal(signal, identity_hrr))):\n",
    "        signal = color_hrrs_internal[color]\n",
    "    \n",
    "    # Calculates max value\n",
    "    max_value = max(value_left_state_x, value_left_state_wm, value_right_state_x, value_right_state_wm)\n",
    "    \n",
    "    # Figures out what to return\n",
    "    if(max_value == value_left_state_x):\n",
    "        return -1, signal\n",
    "    elif(max_value == value_left_state_wm):\n",
    "        return -1, wm\n",
    "    elif(max_value == value_right_state_x):\n",
    "        return 1, signal\n",
    "    elif(max_value == value_right_state_wm):\n",
    "        return 1, wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, inputs, outputs, discount, alpha):\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        self.input = inputs\n",
    "        self.output = outputs\n",
    "        self.weights = np.random.randn(outputs, inputs)\n",
    "        self.bias = 1\n",
    "        \n",
    "    def feedforward(self, X):\n",
    "        self.output = np.dot(self.weights, X) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, state_prime_hrr, state_hrr, y):\n",
    "        if(y == reward_good):\n",
    "            self.error = (reward_bad + self.discount * self.feedforward(state_hrr)) - self.feedforward(state_prime_hrr)\n",
    "            self.weights = np.add(self.weights, (self.alpha * self.error * state_prime_hrr))\n",
    "            self.error = reward_good - self.feedforward(state_hrr)\n",
    "            self.weights = np.add(self.weights, (self.alpha * self.error * state_hrr))\n",
    "        else:\n",
    "            self.error = (reward_bad + self.discount * self.feedforward(state_hrr)) - self.feedforward(state_prime_hrr)\n",
    "            self.weights = np.add(self.weights, (self.alpha * self.error * state_prime_hrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(n, 1, discount, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = random.randint(0, size_of_maze - 1)\n",
    "state_prime = state\n",
    "\n",
    "rand_color = np.random.choice(len(goal_states))\n",
    "goal_state = goal_states[rand_color]\n",
    "signal = color_hrrs_external[rand_color]\n",
    "old_signal = signal\n",
    "wm = identity_hrr\n",
    "\n",
    "for x in range(episodes):\n",
    "        while(state != goal_state):\n",
    "            old_wm = wm\n",
    "            color = rand_color\n",
    "            print(\"Goal:\", goal_state, \"In state: \", state)\n",
    "            if(state == 0):\n",
    "                decision, wm = policy(size_of_maze - 1, 1, wm, color, signal, 1)\n",
    "                if(decision == -1):\n",
    "                    state_prime = state\n",
    "                    state = size_of_maze - 1\n",
    "                else:\n",
    "                    state_prime = state\n",
    "                    state = state + decision\n",
    "            elif(state == size_of_maze - 1):\n",
    "                decision, wm = policy(state - 1, 0, wm, color, signal, 1)\n",
    "                if(decision == -1):\n",
    "                    state_prime = state\n",
    "                    state = state + decision\n",
    "                else:\n",
    "                    state_prime = state\n",
    "                    state = 0\n",
    "            else:\n",
    "                decision, wm = policy(state - 1, state + 1, wm, color, signal, 1)\n",
    "                state_prime = state\n",
    "                state = state + decision\n",
    "            print(wm)\n",
    "            nn.backprop(convolve(convolve(state_hrrs[state_prime], old_wm), old_signal), convolve(convolve(state_hrrs[state], wm), signal), reward_bad)\n",
    "            old_signal = signal\n",
    "            signal = identity_hrr\n",
    "        nn.backprop(convolve(convolve(state_hrrs[state_prime], old_wm), old_signal), convolve(convolve(state_hrrs[state], wm), signal), reward_good)\n",
    "        state = random.randint(0, size_of_maze - 1)\n",
    "        state_prime = state\n",
    "        rand_color = np.random.choice(len(goal_states))\n",
    "        goal_state = goal_states[rand_color]\n",
    "        signal = color_hrrs_external[rand_color]\n",
    "        wm = identity_hrr\n",
    "        print(\"Episode\", x+1, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_signals = np.vstack((identity_hrr, color_hrrs_external))\n",
    "possible_wm = np.vstack((identity_hrr, color_hrrs_internal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "position = np.arange(size_of_maze)\n",
    "value = np.zeros(size_of_maze)\n",
    "for x in range (np.size(possible_signals, 0)):\n",
    "    for y in range (np.size(possible_wm, 0)):\n",
    "        for z in range (size_of_maze):\n",
    "            value[z] = nn.feedforward(convolve(state_hrrs[z], (convolve(possible_signals[x], possible_wm[y]))))\n",
    "        plt.plot(position, value) \n",
    "        print(np.amax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.arange(size_of_maze)\n",
    "value = np.zeros(size_of_maze)\n",
    "for z in range (size_of_maze):\n",
    "    value[z] = nn.feedforward(convolve(state_hrrs[z], possible_wm[2]))\n",
    "plt.plot(position, value) \n",
    "print(np.amax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
