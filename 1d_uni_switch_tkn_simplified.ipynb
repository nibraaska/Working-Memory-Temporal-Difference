{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating hrrs\n",
    "def hrr(length, normalized=False):\n",
    "    if normalized:\n",
    "        x = np.random.uniform(-np.pi,np.pi,int((length-1)/2))\n",
    "        if length % 2:\n",
    "            x = np.real(np.fft.ifft(np.concatenate([np.ones(1), np.exp(1j*x), np.exp(-1j*x[::-1])])))\n",
    "        else:\n",
    "            x = np.real(np.fft.ifft(np.concatenate([np.ones(1), np.exp(1j*x), np.ones(1), np.exp(-1j*x[::-1])])))\n",
    "    else:\n",
    "        x = np.random.normal(0.0, 1.0/np.sqrt(length), length)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve two hrrs\n",
    "def convolve(x, y):\n",
    "    return np.real(np.fft.ifft(np.fft.fft(x)*np.fft.fft(y)))\n",
    "\n",
    "# Pre convolve all hrrs to save time\n",
    "def preconvolve():\n",
    "    preconvolved_matrix = np.zeros([possible_wm.size, size_of_maze, n])\n",
    "    for x in range(len(goal_states)):\n",
    "        for y in range(size_of_maze):\n",
    "            preconvolved_matrix[x][y] = convolve(possible_wm[x], state_hrrs[y])\n",
    "    return preconvolved_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns reward\n",
    "def get_reward(state, goal_state):\n",
    "    return reward_good if state == goal_state else reward_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching between tasks\n",
    "def switch_task(count):\n",
    "    if count < 9:\n",
    "        goal = goal_states[0]\n",
    "        count += 1\n",
    "    elif count == 9:\n",
    "        goal = goal_states[1]\n",
    "        count += 1\n",
    "    elif count > 9 and count < 19:\n",
    "        goal = goal_states[1]\n",
    "        count += 1\n",
    "    elif count == 19:\n",
    "        goal = goal_states[0]\n",
    "        count = 0\n",
    "    return goal, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy for choosing left or right\n",
    "def policy_mov(state, wm, rand_on):\n",
    "    \n",
    "    # Calculates what the left and right positions on the maze are\n",
    "    if (state == 0):\n",
    "        state_left = size_of_maze - 1\n",
    "        state_right = state + 1\n",
    "    elif (state == size_of_maze - 1):\n",
    "        state_left = state - 1\n",
    "        state_right = 0\n",
    "    else:\n",
    "        state_left = state - 1\n",
    "        state_right = state + 1\n",
    "    \n",
    "    # Value of each move\n",
    "    value_left_state_wm = np.dot(weights, preconvolved_matrix[wm][state_left]) + bias\n",
    "    value_right_state_wm = np.dot(weights, preconvolved_matrix[wm][state_right]) + bias\n",
    "    \n",
    "    # Random move\n",
    "    if((np.random.random() < e_soft) and (rand_on == 1)):\n",
    "        return np.random.choice([state_left, state_right])\n",
    "    \n",
    "    # Finds best move\n",
    "    max_value = max(value_left_state_wm, value_right_state_wm)\n",
    "    \n",
    "    # Returns best move\n",
    "    if(max_value == value_left_state_wm):\n",
    "        return state_left\n",
    "    elif(max_value == value_right_state_wm):\n",
    "        return state_right\n",
    "    \n",
    "# Switching wm depending on td error\n",
    "def policy_switch(wm):\n",
    "    return (wm + 1)%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of hrrs\n",
    "n = 1024\n",
    "\n",
    "# Maze creation\n",
    "size_of_maze = 4\n",
    "\n",
    "# Setting goals\n",
    "goal_states = random.sample(range(0, size_of_maze), 2)\n",
    "\n",
    "# Create all state hrrs\n",
    "state_hrrs = np.zeros([size_of_maze, n])\n",
    "for x in range(size_of_maze):\n",
    "    state_hrrs[x] = hrr(n, True)\n",
    "    \n",
    "# Create all wm hrrs\n",
    "possible_wm = np.zeros([len(goal_states), n])\n",
    "for x in range(len(goal_states)):\n",
    "    possible_wm[x] = hrr(n, True)\n",
    "\n",
    "# Reward hrr\n",
    "reward_tkn = hrr(n, True)\n",
    "\n",
    "# Pre convolve hrrs to save time\n",
    "preconvolved_matrix = preconvolve()\n",
    "\n",
    "# Weights and bias of neural network\n",
    "weights = hrr(n, True)\n",
    "bias = 0\n",
    "\n",
    "# Reward for learning\n",
    "reward_bad = 0\n",
    "reward_good = 1\n",
    "\n",
    "# Discounted future rewards\n",
    "discount = 0.5\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# Exploration on or off\n",
    "exp_on = 1\n",
    "\n",
    "# Exploration rate\n",
    "e_soft = 0.01\n",
    "\n",
    "# Treshold to switch wm\n",
    "threshold = -0.50\n",
    "\n",
    "# Number of training cycles\n",
    "episodes = 100000\n",
    "\n",
    "# Steps to try finding the goal before quiting\n",
    "steps_till_quit = 100\n",
    "\n",
    "# For switching tasks\n",
    "count = -1\n",
    "\n",
    "# Frequency to print information\n",
    "print_freq = 1000\n",
    "\n",
    "# wm is set to first\n",
    "wm = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 done\n",
      "Episode 2000 done\n",
      "Episode 3000 done\n",
      "Episode 4000 done\n",
      "Episode 5000 done\n",
      "Episode 6000 done\n",
      "Episode 7000 done\n",
      "Episode 8000 done\n",
      "Episode 9000 done\n",
      "Episode 10000 done\n",
      "Episode 11000 done\n",
      "Episode 12000 done\n",
      "Episode 13000 done\n",
      "Episode 14000 done\n",
      "Episode 15000 done\n",
      "Episode 16000 done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a0ede75a910a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mwm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_switch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtd_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprevious_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(episodes):\n",
    "    \n",
    "    # Current state\n",
    "    current = random.randint(0, size_of_maze - 1)\n",
    "    \n",
    "    # Switching tasks\n",
    "    goal_state, count = switch_task(count)\n",
    "\n",
    "    for y in range (steps_till_quit):\n",
    "        # Gets reward for current state\n",
    "        r = get_reward(current, goal_state)\n",
    "        \n",
    "        # Current state value\n",
    "        current_state = preconvolved_matrix[wm][current]\n",
    "        current_value = np.dot(weights, current_state) + bias\n",
    "        \n",
    "        # Store info about previous state\n",
    "        previous = current\n",
    "        previous_state = current_state\n",
    "        previous_wm = wm\n",
    "        previous_value = current_value\n",
    "        \n",
    "        # Goal reached\n",
    "        if (current == goal_state):\n",
    "            # Get temporal difference error and update weights of neural network\n",
    "            td_error = r - previous_value\n",
    "            weights += (alpha * td_error * convolve(reward_tkn, previous_state))\n",
    "            break\n",
    "        \n",
    "        # What state to move into\n",
    "        move = policy_mov(current, wm, exp_on)\n",
    "        \n",
    "        # Make the move\n",
    "        current = move\n",
    "        current_state = preconvolved_matrix[wm][current]\n",
    "        current_value = np.dot(weights, current_state) + bias\n",
    "\n",
    "        # Weight update for goal not found\n",
    "        td_error = (r + discount*current_value) - previous_value\n",
    "        if td_error < threshold:\n",
    "            wm = policy_switch(wm)\n",
    "            break\n",
    "        weights += (alpha * td_error * previous_state)\n",
    "    if (x+1)%print_freq == 0:\n",
    "        print(\"Episode\", x+1, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.arange(size_of_maze)\n",
    "value = np.zeros(size_of_maze)\n",
    "for z in range (size_of_maze):\n",
    "    if z == goal_states[0]:\n",
    "        value[z] = np.dot(weights, (convolve(reward_tkn, preconvolved_matrix[0][z])))\n",
    "    else:\n",
    "        value[z] = np.dot(weights, preconvolved_matrix[0][z])\n",
    "plt.plot(position, value)\n",
    "print(np.amax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = np.arange(size_of_maze)\n",
    "value = np.zeros(size_of_maze)\n",
    "for z in range (size_of_maze):\n",
    "    if z == goal_states[1]:\n",
    "        value[z] = np.dot(weights, (convolve(reward_tkn, preconvolved_matrix[1][z])))\n",
    "    else:\n",
    "        value[z] = np.dot(weights, preconvolved_matrix[1][z])\n",
    "plt.plot(position, value) \n",
    "print(np.amax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
