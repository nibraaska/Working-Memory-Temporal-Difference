{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time, sys, random, pylab, os, glob, math\n",
    "\n",
    "from math import fabs\n",
    "from random import randrange\n",
    "from random import choice\n",
    "from IPython.display import clear_output\n",
    "from sys import argv\n",
    "from os import path\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import JSONLogger\n",
    "from bayes_opt import Events\n",
    "from bayes_opt.util import load_logs\n",
    "\n",
    "from hrr import *\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_steps(start, goal, size_of_maze):\n",
    "    opt = abs(goal - start)\n",
    "    if opt > size_of_maze / 2:\n",
    "        opt = size_of_maze - opt\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hrr_string(wm, signal, state, atr):\n",
    "    if wm == \"I\" and signal == \"I\":\n",
    "        return \"State:\" + str(state) + \"*\" + \"Atr:\" + str(atr)\n",
    "    elif wm == \"I\":\n",
    "        return \"Signal:\" + str(signal) + \"*\" + \"State:\" + str(state) + \"*\" + \"Atr:\" + str(atr)\n",
    "    elif signal == \"I\":\n",
    "        return \"WM:\" + str(wm) + \"*\" + \"State:\" + str(state) + \"*\" + \"Atr:\" + str(atr)\n",
    "    else:\n",
    "        return \"WM:\" + str(wm) + \"*\" + \"Signal:\" + str(signal) + \"*\" + \"State:\" + str(state) + \"*\" + \"Atr:\" + str(atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moves(state, size_of_maze):\n",
    "    if(state == 0):\n",
    "        return size_of_maze - 1, 1\n",
    "    elif(state == size_of_maze - 1):\n",
    "        return size_of_maze - 2, 0\n",
    "    else:\n",
    "        return state - 1, state + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_policy(goal, moves, wms, signals, atr, rand_on, debug, weights, bias, ltm, e_soft):\n",
    "    val = -9999\n",
    "    for move in moves:\n",
    "        for wm in list(dict.fromkeys(wms + [\"I\"])):\n",
    "            for signal in list(dict.fromkeys(signals + [\"I\"])):\n",
    "                if move == goal:\n",
    "                    encode_str = build_hrr_string(wm, signal, str(move) + reward_tkn(), atr)\n",
    "                else:\n",
    "                    encode_str = build_hrr_string(wm, signal, move, atr)\n",
    "                if (debug):\n",
    "                    print(encode_str)\n",
    "                temp = np.dot(weights, ltm.encode(encode_str)) + bias\n",
    "                if debug:\n",
    "                    if signal != \"I\":\n",
    "                        print(\"Move: {0}, WM: {1}, Signal: {2}In, Atr: {3}, Value: {4}\".format(move, wm, signal, atr, temp))\n",
    "                    else:\n",
    "                        print(\"Move: {0}, WM: {1}, Signal: {2}, Atr: {3}, Value: {4}\".format(move, wm, signal, atr, temp))\n",
    "                if temp > val:\n",
    "                    val = temp\n",
    "                    s_move = move\n",
    "                    if signal != \"I\":\n",
    "                        s_wm = signal + \"In\"\n",
    "                    else:\n",
    "                        s_wm = wm\n",
    "    if(np.random.random_sample() < e_soft) and rand_on:\n",
    "        if(debug):\n",
    "            print(\"RANDOM MOVE\")\n",
    "        return (np.random.choice(moves), wms[0], atr, True)\n",
    "    \n",
    "    return (s_move, s_wm, atr, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logmod(x):\n",
    "    return np.sign(x)*np.log(abs(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_tkn():\n",
    "    return \"*rewardTkn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_policy_negative(atr, num_of_atrs):\n",
    "    return np.random.randint(low=0, high=num_of_atrs)\n",
    "\n",
    "def context_policy_positive(wm, signal, state, atr, num_of_atrs, weights, ltm, bias):\n",
    "    val = -9999\n",
    "    for atr in range(0, num_of_atrs):\n",
    "        encode_str = build_hrr_string(wm, signal, state, atr)\n",
    "        temp = np.dot(weights, ltm.encode(encode_str)) + bias\n",
    "        if temp > val:\n",
    "            val = temp\n",
    "            s_atr = atr\n",
    "    return s_atr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress, episode):\n",
    "    bar_length = 50\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Episode {0}, Progress: [{1}] {2:.1f}%\".format(episode, \"=\" * block + \".\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(method, seed_val, num_of_atrs, atr_values, threshold, hrr_length, ltm, weights, eligibility, reward_good, dynamic_threshold, normalized):\n",
    "\n",
    "#     seed(seed_val)\n",
    "    num_of_atrs += 1\n",
    "    atr_values = [1 * reward_good] * num_of_atrs\n",
    "    if dynamic_threshold:\n",
    "        threshold = 1\n",
    "        \n",
    "    if method == 1:\n",
    "        hrr_length = (num_of_atrs * hrr_length) / (num_of_atrs - 1)\n",
    "        store_old = ltm.getStore()\n",
    "        weights_new = hrr(int(hrr_length), normalized)\n",
    "        ltm_new = LTM(int(hrr_length), normalized)\n",
    "\n",
    "        inv = np.linalg.pinv(np.atleast_2d(weights_new))\n",
    "        for key in store_old.keys():\n",
    "            key_val = store_old[key]\n",
    "            val = np.dot(weights, key_val)\n",
    "            guess = np.dot(inv, val).ravel()\n",
    "            ltm_new.encode_val(key, guess)      \n",
    "        \n",
    "    elif method == 2:\n",
    "        hrr_length = (num_of_atrs * hrr_length) / (num_of_atrs - 1)\n",
    "        store_old = ltm.getStore()\n",
    "        ltm_new = LTM(int(hrr_length), normalized)\n",
    "\n",
    "        new_hrrs = hrrs(hrr_length, ltm.count(), normalized)\n",
    "        vals = []\n",
    "        for key in store_old.keys():\n",
    "            key_val = store_old[key]\n",
    "            vals += [np.dot(weights, key_val)]  \n",
    "        s = np.linalg.pinv(new_hrrs)\n",
    "        weights_new = np.asarray(np.dot(s,np.atleast_2d(vals).T)).ravel()\n",
    "\n",
    "        i = 0\n",
    "        for key in store_old.keys():\n",
    "            ltm_new.encode_val(key, new_hrrs[i])\n",
    "            i+=1\n",
    "            \n",
    "    ltm = ltm_new\n",
    "    weights = weights_new\n",
    "    eligibility = np.zeros(int(hrr_length))\n",
    "\n",
    "    return num_of_atrs, atr_values, threshold, hrr_length, ltm, weights, eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_testing(): \n",
    "    testing = True\n",
    "    rand_on = 0\n",
    "    alpha = 0.01\n",
    "    threshold_alpha = 0\n",
    "    atr_alpha = 0\n",
    "    return testing, rand_on, alpha, threshold_alpha, atr_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(method_val, episodes_val, hrr_length_val, steps_till_quit_val, non_obs_task_switch_rate_val, discount_val, alpha_val, atr_alpha_val, atr_threshold_val, threshold_alpha_val, e_soft_val, eli_lambda_val):\n",
    "    \n",
    "    seed_val = 1343765\n",
    "    seed(seed_val)\n",
    "    \n",
    "    if method_val < 0.5:\n",
    "        method = 1\n",
    "    else:\n",
    "        method = 2\n",
    "    episodes = math.floor(episodes_val)\n",
    "\n",
    "    hrr_length = math.floor(hrr_length_val)\n",
    "    steps_till_quit = math.floor(steps_till_quit_val)\n",
    "    non_obs_task_switch_rate = math.floor(non_obs_task_switch_rate_val)\n",
    "    \n",
    "    normalized = True\n",
    "    \n",
    "    signals = [\"I\"]\n",
    "    goals = [[0], [4], [7], [10], [13]]\n",
    "    size_of_maze = 20\n",
    "    \n",
    "    num_non_obs_tasks = len(goals)\n",
    "    num_obs_tasks = len(signals)\n",
    "    input_size = hrr_length\n",
    "    output_size = 1\n",
    "    \n",
    "    discount = discount_val\n",
    "    alpha = alpha_val\n",
    "    \n",
    "    reward_bad = -1\n",
    "    reward_good = 0\n",
    "    \n",
    "    num_of_atrs = 1\n",
    "    atr_alpha = atr_alpha_val\n",
    "    atr_values = (np.ones(num_of_atrs) * reward_good).tolist()\n",
    "    atr_threshold = -atr_threshold_val\n",
    "    \n",
    "    threshold = 1\n",
    "    threshold_alpha = threshold_alpha_val\n",
    "    dynamic_threshold = True\n",
    "    \n",
    "    e_soft = e_soft_val\n",
    "    rand_on = 1\n",
    "    eli_lambda = eli_lambda_val\n",
    "    \n",
    "    weights = hrr(hrr_length, normalized)\n",
    "    bias = 1\n",
    "    eligibility = np.zeros(hrr_length)\n",
    "    \n",
    "    percent_check = 9\n",
    "    non_obs = 0\n",
    "    current_atr = 0\n",
    "    current_wm = \"I\"\n",
    "    \n",
    "    changed = False\n",
    "    debug = False\n",
    "    testing = False\n",
    "    create_plots = True\n",
    "    episodic_memory = False\n",
    "    step_store = []\n",
    "    if create_plots:\n",
    "        pos_err_store = []\n",
    "        neg_err_store = []\n",
    "        total_error = []\n",
    "        total_goal_error = []\n",
    "        switch_error = []\n",
    "        norm_error = []\n",
    "        threshold_vals = []\n",
    "    live_graph = False\n",
    "    ltm = LTM(hrr_length, normalized)\n",
    "    \n",
    "    for x in range(episodes):\n",
    "\n",
    "        current_state = random.randint(0, size_of_maze - 1)\n",
    "        start = current_state\n",
    "        current_signal = np.random.choice(signals)\n",
    "        eligibility *= 0.0\n",
    "\n",
    "        if episodic_memory:\n",
    "            episode_memory = []\n",
    "\n",
    "        changed = False\n",
    "\n",
    "        if x%non_obs_task_switch_rate == 0:\n",
    "            non_obs = choice([i for i in range(len(goals)) if i not in [non_obs]])\n",
    "            changed = True\n",
    "        if num_obs_tasks == 1:\n",
    "            goal = goals[non_obs][0]\n",
    "        else:\n",
    "            goal = goals[non_obs][signals.index(current_signal)]\n",
    "\n",
    "        steps = 0\n",
    "        opt_steps = get_opt_steps(current_state, goal, size_of_maze)\n",
    "\n",
    "        if testing == False and x > ((episodes*percent_check) / 10):\n",
    "            testing, rand_on, alpha, threshold_alpha, atr_alpha = start_testing()\n",
    "\n",
    "        for y in range(steps_till_quit):\n",
    "            if create_plots:\n",
    "                threshold_vals += [threshold]\n",
    "            if (current_state == goal):\n",
    "                encode_str = build_hrr_string(current_wm, current_signal, str(current_state) + reward_tkn(), current_atr)\n",
    "                goal_hrr = ltm.encode(encode_str)\n",
    "                goal_value = np.dot(weights, goal_hrr) + bias\n",
    "\n",
    "                if episodic_memory:\n",
    "                    episode_memory += [[current_state, goal_value, goal]]\n",
    "\n",
    "                error = reward_good - goal_value\n",
    "                eligibility *= eli_lambda\n",
    "                eligibility = eligibility + goal_hrr\n",
    "                weights = np.add(weights, (alpha * logmod(error) * eligibility))\n",
    "\n",
    "                if dynamic_threshold:\n",
    "                    threshold += threshold_alpha * logmod(error)\n",
    "\n",
    "                atr_values[current_atr] += atr_alpha * logmod(error)\n",
    "\n",
    "                if create_plots:\n",
    "                    total_goal_error += [error]\n",
    "\n",
    "                if(debug):\n",
    "                    print(\"In goal with value {0}\".format(goal_value))\n",
    "\n",
    "                break\n",
    "    \n",
    "            previous_wm = current_wm\n",
    "            previous_signal = current_signal\n",
    "            previous_state = current_state\n",
    "            previous_atr = current_atr\n",
    "\n",
    "            if debug:\n",
    "                print(\"Previous WM:, {0}, Signal:, {1}, State, {2}, ATR:, {3}\".format(previous_wm, previous_signal, previous_state, previous_atr))\n",
    "\n",
    "            encode_str = build_hrr_string(previous_wm, previous_signal, previous_state, previous_atr)\n",
    "            previous_state_hrr = ltm.encode(encode_str)\n",
    "            previous_value = np.dot(weights, previous_state_hrr) + bias\n",
    "\n",
    "            if debug:\n",
    "                print(\"Started with state: {0}, State Value: {1}, WM: {2},  Atr: {3}\".format(previous_state, previous_value, previous_wm, previous_atr))\n",
    "\n",
    "            current_signal = \"I\"\n",
    "            left, right = get_moves(previous_state, size_of_maze)\n",
    "            if previous_signal != \"I\":\n",
    "                previous_signal += \"In\"\n",
    "\n",
    "            move, wm, current_atr, random_move = move_policy(goal, [left, right], [previous_wm, previous_signal], [current_signal], previous_atr, rand_on, debug, weights, bias, ltm, e_soft)\n",
    "            steps += 1\n",
    "            current_wm = wm\n",
    "            current_state = move\n",
    "\n",
    "            if random_move:\n",
    "                eligibility *= 0.0\n",
    "\n",
    "            if(debug):\n",
    "                print(\"Moves {0}, taken {1}\".format([left, right], move))\n",
    "\n",
    "            if debug:\n",
    "                print(\"Current WM {0}, Current Signal {1}, Current state {2}, Current ATR {3}\".format(current_wm, current_signal, current_state, current_atr))\n",
    "\n",
    "            if current_state == goal:\n",
    "                encode_str = build_hrr_string(current_wm, current_signal, str(current_state) + reward_tkn(), current_atr)     \n",
    "                if debug:\n",
    "                    print(\"In goal: WM: {1}, ATR: {2}\".format(current_wm, current_atr))\n",
    "            else:\n",
    "                encode_str = build_hrr_string(current_wm, current_signal, current_state, current_atr)\n",
    "\n",
    "            current_state_hrr = ltm.encode(encode_str)\n",
    "            current_value = np.dot(weights, current_state_hrr) + bias\n",
    "\n",
    "            sarsa_error = (reward_bad + discount * current_value) - previous_value\n",
    "            eligibility *= eli_lambda\n",
    "            eligibility = eligibility + previous_state_hrr\n",
    "            weights = np.add(weights, (alpha * logmod(sarsa_error) * eligibility))\n",
    "\n",
    "            atr_values[current_atr] += atr_alpha * logmod(sarsa_error)\n",
    "\n",
    "            if dynamic_threshold:\n",
    "                threshold += threshold_alpha * logmod(sarsa_error)\n",
    "\n",
    "            if create_plots:\n",
    "                total_error += [sarsa_error]\n",
    "                norm_error += [sarsa_error]\n",
    "\n",
    "            if sarsa_error > fabs(threshold) or sarsa_error < -fabs(threshold):\n",
    "\n",
    "                if np.mean(atr_values) < atr_threshold:\n",
    "                    num_of_atrs, atr_values, threshold, hrr_length, ltm, weights, eligibility = reset(method, seed_val, num_of_atrs, atr_values, threshold, hrr_length, ltm, weights, eligibility, reward_good, dynamic_threshold, normalized)\n",
    "\n",
    "                if create_plots:\n",
    "                    switch_error += [sarsa_error]\n",
    "\n",
    "                if create_plots:\n",
    "                    if testing and sarsa_error > fabs(threshold):\n",
    "                        pos_err_store += [sarsa_error]\n",
    "                    elif testing and sarsa_error < -fabs(threshold):\n",
    "                        neg_err_store += [sarsa_error]\n",
    "\n",
    "                if sarsa_error > fabs(threshold):\n",
    "                    current_atr = context_policy_positive(current_wm, current_signal, current_state, current_atr, num_of_atrs, weights, ltm, bias)\n",
    "                elif sarsa_error < -fabs(threshold):\n",
    "                    current_atr = context_policy_negative(previous_atr, num_of_atrs)\n",
    "\n",
    "                eligibility *= 0.0\n",
    "\n",
    "                if changed:\n",
    "                    steps = 0\n",
    "                    start = current_state\n",
    "                    opt_steps = get_opt_steps(current_state, goal, size_of_maze)\n",
    "\n",
    "                if(debug):\n",
    "                    print(\"Changed atr from {0} to {1}\".format(previous_atr, current_atr))\n",
    "\n",
    "            if debug:\n",
    "                input(\"\")\n",
    "\n",
    "        if testing:\n",
    "            if current_state == goal:\n",
    "                step_store += [steps - opt_steps]\n",
    "            else:\n",
    "                step_store += [steps_till_quit]\n",
    "                \n",
    "    return (len(step_store)-np.count_nonzero(step_store))*100.0 / len(step_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'method_val': (0.6, 0.9),\n",
    "    'episodes_val': (5000, 100000),\n",
    "    'hrr_length_val': (1024, 6144),\n",
    "    'steps_till_quit_val': (100, 200),\n",
    "    'non_obs_task_switch_rate_val': (250, 500),\n",
    "    'discount_val': (0.1, 0.7),\n",
    "    'alpha_val': (0.01, 0.2),\n",
    "    'atr_alpha_val': (0.0001, 0.0003),\n",
    "    'atr_threshold_val': (0.3, 0.5),\n",
    "    'threshold_alpha_val': (0.0001, 0.0002),\n",
    "    'e_soft_val': (0.0001, 0.0005),\n",
    "    'eli_lambda_val': (0, 0.2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=run,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | alpha_val | atr_al... | atr_th... | discou... | e_soft... | eli_la... | episod... | hrr_le... | method... | non_ob... | steps_... | thresh... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.08923418089348906, 0.00024406489868843161, 0.30002287496346897, 0.28139954357910385, 0.00015870235632684524, 0.018467718953759562, 22694.720080878735, 2793.2709224604046, 0.719030242269201, 384.70418350083924, 141.91945144032948, 0.00016852195003967595)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e5d9647802bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# optimizer.subscribe(Events.OPTMIZATION_STEP, logger)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mload_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/home/nibraas/Coding/Mingo/logs/wm_logs_2_method_2.json\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/research/lib/python3.8/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-550cd2dba237>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(method_val, episodes_val, hrr_length_val, steps_till_quit_val, non_obs_task_switch_rate_val, discount_val, alpha_val, atr_alpha_val, atr_threshold_val, threshold_alpha_val, e_soft_val, eli_lambda_val)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mprevious_signal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"In\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_atr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprevious_wm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_signal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcurrent_signal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_atr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mltm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_soft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mcurrent_wm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bb356e47f3ea>\u001b[0m in \u001b[0;36mmove_policy\u001b[0;34m(goal, moves, wms, signals, atr, rand_on, debug, weights, bias, ltm, e_soft)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mltm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Coding/Working-Memory-Temporal-Difference/combined_model/all_stats/dynamic_t_non_obs_stats/newStats/hrr.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubstr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0msubstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# logger = JSONLogger(path=\"/home/nibraas/Coding/Mingo/logs/wm_logs_2_method_2.json\")\n",
    "# optimizer.subscribe(Events.OPTMIZATION_STEP, logger)\n",
    "load_logs(optimizer, logs=[\"/home/nibraas/Coding/Mingo/logs/wm_logs_2_method_2.json\"])\n",
    "optimizer.maximize(n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'target': 93.7748344370861,\n",
    "#  'params': {'alpha_val': 0.1198577662209101,\n",
    "#   'atr_alpha_val': 0.0001730660106626197,\n",
    "#   'atr_threshold_val': 0.45555336687067005,\n",
    "#   'discount_val': 0.5828716209960245,\n",
    "#   'e_soft_val': 0.0005,\n",
    "#   'eli_lambda_val': 0.13055952287831313,\n",
    "#   'episodes_val': 7559.054944609811,\n",
    "#   'hrr_length_val': 4696.480303672614,\n",
    "#   'method_val': 0.9655270732767963,\n",
    "#   'non_obs_task_switch_rate_val': 372.6904809738739,\n",
    "#   'steps_till_quit_val': 159.86389051414127,\n",
    "#   'threshold_alpha_val': 0.0001401220235475343}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
